# Regression

- **Linear Regression:** Model continuous targets using a linear combination of features. Model form: $y \approx \beta_0 + \beta_1 x_1 + \cdots + \beta_d x_d$ ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://file-gunh7sjfxuf6za3x3mmcxm%23:~:text=multiple%20linear%20regression%20model:/)) ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://xn--file-gunh7sjfxuf6za3x3mmcxm%23:~:text=%20predicted%20value%20for%20the,dxid-hi6cu086u/)). Fit parameters by minimizing **Mean Squared Error (MSE)**: $L(\beta_0,\ldots,\beta_d)=\sum_{i=1}^n (y_i - \beta_0 - \sum_{j=1}^d \beta_j x_{ij})^2$ ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=%20loss%20function:%20l,n-f075bo4m/)) ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=n%20i=1%20,2-l117a/)). The optimal solution (normal equation) is $\hat{\boldsymbol{\beta}}=(X^T X)^{-1}X^T \mathbf{y}$ for full rank $X$.
- **Multiple Linear Regression:** Same idea with multiple features – often include a constant feature 1 for $\beta_0$ ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://file-gunh7sjfxuf6za3x3mmcxm%23:~:text=linear%20algebra%20review%20from%20last,multiple%20linear%20regression%20model%20selection/)) ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://file-gunh7sjfxuf6za3x3mmcxm%23:~:text=x11%20x12%20,x1d/)). Predicted values in vector form: $\hat{\mathbf{y}} = X\boldsymbol{\beta}$ ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://xn--file-gunh7sjfxuf6za3x3mmcxm%23:~:text=%20predicted%20value%20for%20the,dxid-hi6cu086u/)) ([Lecture 2 — Multiple Linear Regression Annotated.pdf](file://xn--file-gunh7sjfxuf6za3x3mmcxm%23:~:text=%20predicted%20values%20are%20given,by:%20%20=%20x-n9r2454azn95b/)). Adding more features (e.g. polynomial terms) can only _decrease or maintain_ the minimum training loss, never increase it (more complex model can fit at least as well as simpler) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,x1,%20x2,%20x/)) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=always%20sometimes%20never/)).
- **Analytic Solution vs. Gradient:** For OLS, we can solve directly or use iterative optimization. _Example:_ Gradient of MSE is $-2X^T(\mathbf{y}-X\boldsymbol{\beta})$; gradient descent updates $\boldsymbol{\beta}\leftarrow\boldsymbol{\beta} + 2\eta,X^T(\mathbf{y}-X\boldsymbol{\beta})$ (with step size $\eta$) until convergence.
- **Alternate Loss Functions:** Choice of loss affects the “best” predictor and robustness to outliers. _L2 loss_ (squared error) is minimized by the data mean for a constant model ([Homework 1 ML — Questions.pdf](file://xn--file-pkupuf1w2mpbw2yetwspmx%23:~:text=,m\)%20=%20n-d146a/)) ([Homework 1 ML — Questions.pdf](file://file-pkupuf1w2mpbw2yetwspmx%23:~:text=,about%20the%20minimization%20problem%20directly/)), but is sensitive to outliers. _L1 loss_ (absolute error) is minimized by the median ([Homework 1 ML — Questions.pdf](file://xn--file-pkupuf1w2mpbw2yetwspmx%23:~:text=,m\)%20=%20n-d146a/)) ([Homework 1 ML — Questions.pdf](file://file-pkupuf1w2mpbw2yetwspmx%23:~:text=i=1%20,derivatives%20might%20not%20be%20helpful/)) – more robust to outliers than mean. _L∞ loss_ (max error) is minimized by the midrange (midpoint of min and max), focusing only on the worst error. **Outlier Robustness:** Median (L1) is generally more robust than mean (L2), whereas L∞ is least robust (one large error dominates) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=it%20is%20therefore%20less%20robust,of%20the%20worst%20data%20example/)) ([midterm_practice_soln - Copy.pdf](file://xn--file-gyheq2b7g3xp96bhewpdtk%23:~:text=,when%20compared%20to%20l%20loss-y321c/)).

# Classification

- **Overview:** Predict categorical labels (binary or multi-class) from feature vectors ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=classification/)) ([lec5_annotated.pdf](file://xn--file-gxo8jelz8d5ggzceztqssj%23:~:text=%20multi,classification-3434b/)). Classifiers can be **discriminative** (directly model $y|x$) or **generative** (model joint distribution of $x$ and $y$). Key methods: **Logistic Regression** and **Naïve Bayes**.
- **Logistic Regression:** A linear **probabilistic** classifier ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=logistic%20regression/)). Uses the sigmoid function to output a probability: $P(y=1|x) = \frac{1}{1+e^{-z}}$ where $z = \mathbf{w}^T x + b$. It’s a linear decision boundary in feature space ([lec5_annotated.pdf](file://xn--file-gxo8jelz8d5ggzceztqssj%23:~:text=%20logistic%20regression%20is%20a,classification%20method-gv97d/)). Prediction $\hat{y}=1$ if $P(y=1|x) \ge 0.5$ (i.e. if $z>0$). Trained by maximizing likelihood (equivalently minimizing **logistic loss**): $L(\mathbf{w})=-\sum_{i}[y_i\ln h_{\mathbf{w}}(x_i)+(1-y_i)\ln(1-h_{\mathbf{w}}(x_i))]$, which is convex. No closed-form solution; use gradient descent or other optimizers. _Note:_ Scaling the weights $\mathbf{w},b$ by a constant does not change the classification boundary (only the confidence); e.g. multiplying $\mathbf{w},b$ by $\alpha>0$ leaves predictions $\hat{y}$ unchanged, though predicted probabilities become more extreme as $\alpha$ grows (closer to 0 or 1).
- **Bayes Classifier:** Assigns label $\hat{y}=\arg\max_y P(Y=y|X=x)$ using **Bayes’ Rule:** $P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$. For two classes, compare the _posterior_ odds. _Example:_ Gaussian model per class: $\ Pr(Y=1|x) = \frac{P(x|Y=1)P(Y=1)}{P(x|Y=0)P(Y=0)+P(x|Y=1)P(Y=1)}$. Often we take logs and compare log-odds to a threshold. In practice, if $P(Y=1|x)\ge 0.5$ we predict class 1 (equivalent to $P(x|Y=1)P(Y=1)\ge P(x|Y=0)P(Y=0)$ for 2-class case) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=ratio%20rnew,and%20outputs%20s0%20or%20s1/)) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=pr\(s0%20,rnew/)).
- **Naïve Bayes:** A practical Bayes classifier assuming feature independence given the class. Two variants for text: **Bernoulli NB** (features are binary indicators for word occurrence) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=categorization%20problems,the%20document,%20and%200%20otherwise/)) and **Multinomial NB** (features are word counts) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=multinomial%20naive%20bayes%20uses%20the,of%20words%20in%20category%20c/)). Compute $P(\text{class}| \text{features}) \propto P(\text{class})\prod_j P(\text{feature}_j|\text{class})$ multiplying likelihoods of each feature. Use **Laplace smoothing** to avoid zero probabilities: add 1 to all counts when estimating probabilities ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=in%20both%20versions%20of%20naive,case%20we%20will%20use%201/)) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=the%20class%20spam,2857142857142857-o620cea/)) (for multinomial: $P(\text{word}|class) = \frac{\text{count(word,class)}+1}{\text{total words in class} + |Vocab|}$ ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=so%20for%20example%20in%20multinomial,2857142857142857/))). Naïve Bayes is simple, fast, and often works well for high-dimensional sparse data (e.g. text).
- **Evaluation Metrics:** For binary classification, use the confusion matrix counts: True Positive (TP), True Negative (TN), False Positive (FP), False Negative (FN).
    - **Accuracy:** Proportion of correct predictions = $\frac{\text{TP+TN}}{\text{Total}}$ ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,75/)).
    - **Precision:** $\frac{\text{TP}}{\text{TP + FP}}$ – of predicted positives, how many were actual positives ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,857/)).
    - **Recall:** $\frac{\text{TP}}{\text{TP + FN}}$ – of actual positives, how many were correctly predicted ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,75/)).
    - **F1-Score:** Harmonic mean of precision & recall = $\frac{2 \cdot \text{Precision}\cdot \text{Recall}}{\text{Precision}+\text{Recall}}$ ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=6%2F8%20=%200/)).  
        _These metrics help assess classification beyond simple accuracy, especially on imbalanced data._

# Decision Trees & Ensemble Methods

- **Decision Trees:** Hierarchical, tree-structured classifiers/regressors that recursively split the data based on feature values ([lec7 - Decision Trees and Forest.pdf](file://file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=decision%20trees/)). Each internal node splits on an attribute, each branch corresponds to an outcome of the test, and each leaf node outputs a prediction (class label for classification, or a numeric value for regression) ([lec7 - Decision Trees and Forest.pdf](file://file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=how%20to%20make%20predictions%3F/)) ([lec7 - Decision Trees and Forest.pdf](file://file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=output:%20predicted:%20y/)). To predict, _drop_ an example down the tree following the splits until a leaf is reached ([lec7 - Decision Trees and Forest.pdf](file://file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=how%20to%20make%20predictions%3F/)).
    - **Splitting Criterion:** Choose splits that maximize reduction in impurity. Common impurity measures: **Entropy** $H(D) = -\sum_{k} p_k \log_2 p_k$ ([lec7 - Decision Trees and Forest.pdf](file://xn--file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=entropy:%20whats%20the%20smallest%20possible,xj-m618c/)) (with $p_k$ the fraction of examples of class $k$ in the node) and **Gini index** $G(D)=\sum_k p_k(1-p_k)$. **Information Gain (IG)** from a split is the drop in entropy: $IG = H(\text{parent}) - \frac{N_L}{N}H(\text{left child}) - \frac{N_R}{N}H(\text{right child})$. A higher IG means the attribute better separates the classes ([lec7 - Decision Trees and Forest.pdf](file://file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=information%20gain/)). For regression trees, typically use reduction in Residual Sum of Squares (RSS) as the criterion (choose split that minimizes MSE in children).
    - **Tree Growth:** Grows until leaves are pure or some stopping criterion (min samples or max depth) is met. Unrestricted trees can overfit (memorize training data patterns, including noise).
    - **Pruning:** Cutting back the tree to prevent overfitting. **Cost-complexity pruning:** add a penalty $\alpha \times (\text{number of leaf nodes})$ to the tree’s training error and prune branches that minimally increase this cost ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=and%20rr,%20based%20on%20a,s/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=,to%20select%20the%20optimal%20subtree/)). Use a validation set or **cross-validation** to find the optimal $\alpha$ (complexity parameter) that gives best generalization ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=,to%20select%20the%20optimal%20subtree/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=criterion%20used%20in%20pruning%20a,to%20select%20the%20optimal%20subtree/)). Pruned trees are smaller (fewer leaves) and often generalize better.
- **Ensemble Methods:** Combine multiple models (often multiple trees) to improve performance.
    - **Bagging (Bootstrap Aggregating):** Build many trees on random bootstrapped subsets of the training data and average their predictions. Bagging reduces **variance** – the average of many overfit trees tends to cancel out noisy idiosyncrasies ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=,in%20your%20discussion,%20address/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=i,compared%20to%20a%20single%20tree/)). Each tree sees a different sample, and for classification a majority vote is taken.
    - **Random Forest:** An improvement over bagging for trees. In addition to bootstrapping data, at each split only a random subset of features is considered ([lec7 - Decision Trees and Forest.pdf](file://xn--file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=random%20forest%20%20as%20with,tree%20for%20each%20bootstrap%20sampling-gm60g/)). This decorrelates the trees (prevents all trees from splitting on the same dominant feature) and further reduces variance in the ensemble ([lec7 - Decision Trees and Forest.pdf](file://xn--file-9zgjti8vqkrdzyjk9z8fm3%23:~:text=random%20forest%20%20as%20with,tree%20for%20each%20bootstrap%20sampling-gm60g/)). A random forest often achieves better accuracy than a single tree while maintaining low correlation among trees.
    - **Boosting:** Sequentially build an ensemble by training each new model to correct the errors of the previous ones. For example, **AdaBoost** starts with a weak tree and iteratively adds trees, weighting data points higher if they were misclassified by earlier trees. Each added tree focuses on the “hard” examples, and final predictions are a weighted vote of all trees. **Gradient boosting** (e.g. XGBoost) generalizes this by fitting new trees to the residuals (gradient of loss) of the current ensemble. Boosting tends to reduce **bias** (can fit complex functions) but may increase variance; it can overfit if too many rounds are used, so regularization (shrinkage, limiting depth, early stopping) is applied. _Trade-off:_ A single tree is easy to interpret, while ensembles (bagging, RF, boosting) sacrifice interpretability for higher predictive accuracy ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=,in%20your%20discussion,%20address/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=i,compared%20to%20a%20single%20tree/)). Ensembles typically yield more robust models (lower generalization error) at the cost of model complexity.

# Gradient Descent & Optimization

- **Gradient Descent (GD):** An iterative algorithm to minimize a differentiable loss function $L(\boldsymbol{\theta})$ ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=gradient%20descent:%20a%20greedy%20search,that%20often/)). Initialize parameters $\boldsymbol{\theta}^{(0)}$ (e.g. randomly or zeros). Then update iteratively: $\boldsymbol{\theta}^{(t+1)} = \boldsymbol{\theta}^{(t)} - \eta \nabla_{!\boldsymbol{\theta}}L(\boldsymbol{\theta}^{(t)})$, where $\eta$ is the learning rate (step size). Move $\boldsymbol{\theta}$ in the direction of steepest descent (negative gradient) to reduce the loss ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=gradient%20descent:%20a%20greedy%20search,that%20often/)). Repeat until convergence (or for a fixed number of iterations). GD will approach a local minimum; if $L$ is convex (like linear regression MSE or logistic loss), it converges to the global minimum ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=convergence%20of%20gradient%20descent/)).
    - **Learning Rate:** Critical hyperparameter. If $\eta$ is too large, GD may overshoot or diverge; if too small, convergence is _slow_ ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,converge%20to%20a%20local%20minimum/)). Often a schedule or tuning is needed for efficiency. Decreasing $\eta$ generally slows convergence (steps are smaller), but ensures more stable approach to minimum ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,converge%20to%20a%20local%20minimum/)).
    - **Stochastic Gradient Descent (SGD):** A variant using one (or a mini-batch of) training example(s) to update $\boldsymbol{\theta}$ at a time, rather than the full gradient on all data. Each update $\boldsymbol{\theta} \leftarrow \boldsymbol{\theta} - \eta \nabla_{!\boldsymbol{\theta}} L_i(\boldsymbol{\theta})$ uses the gradient from a single example $i$ (or a mini-batch). This introduces noise in updates but drastically reduces computation per step, often leading to faster convergence in terms of epochs. SGD oscillates around the minimum but can escape shallow local minima and scales well to large datasets.
    - **Convergence:** For convex $L$, GD with a sufficiently small constant $\eta$ will converge to the global minimum ([lec5_annotated.pdf](file://file-gxo8jelz8d5ggzceztqssj%23:~:text=convergence%20of%20gradient%20descent/)). In practice, one may use techniques like learning rate decay, momentum, or adaptive methods (Adam, etc.) to improve convergence speed. For non-convex problems (neural nets), GD may get stuck in local minima or saddle points, but often still finds acceptable solutions.

# Overfitting, Regularization & Model Selection

- **Underfitting vs. Overfitting:** _Underfitting_ occurs when a model is too simple to capture the underlying pattern (high bias) ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=why%20is%20model%20selection%20important%3F/)). It yields high training error as well as high test error. _Overfitting_ occurs when a model is too complex and learns noise or random fluctuations (high variance) ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=why%20is%20model%20selection%20important%3F/)). It has very low training error but high test (generalization) error. The goal is a model with the right complexity to minimize test error ([lec4_annotated.pdf](file://xn--file-5vhyfd1zvbldtluwvaeuz8%23:~:text=%20underfitting:%20model%20is%20too,underlying%20structure%20of%20the%20data-xl11f/)). Typically, training loss decreases with model complexity, while validation/test loss decreases then eventually rises when overfitting sets in ([Lecture 3 — Annotated.pdf](file://file-8teyxesc7wj3ah3hbg6dhm%23:~:text=train/)) ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=%20train%20loss%20continues%20to,decrease%20as%20model%20complexity%20grows-9n40f/)).
- **Generalization & Data Splitting:** Always evaluate on data not used for training. Use a **train/test split** (e.g. 80/20) to estimate generalization performance ([Lecture 3 — Annotated.pdf](file://file-8teyxesc7wj3ah3hbg6dhm%23:~:text=train/)) ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=%20train%20q%20models%20f,xtrain,ytrain-m245c/)). Sometimes also carve out a **validation set** (or use **cross-validation**) to tune hyperparameters (model complexity) ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=%20evaluate%20loss%20of%20each,xtest,ytest-lv57c/)) ([Lecture 3 — Annotated.pdf](file://file-8teyxesc7wj3ah3hbg6dhm%23:~:text=from%20last%20class%20model%20selection,bias%20and%20variance%20probabilistic%20modeling/)). **K-fold Cross-Validation:** partition data into $K$ folds, train on $K-1$ folds and validate on the remaining 1, repeat $K$ times, and average results to select the best model ([Lecture 3 — Annotated.pdf](file://file-8teyxesc7wj3ah3hbg6dhm%23:~:text=from%20last%20class%20model%20selection,bias%20and%20variance%20probabilistic%20modeling/)) ([Lecture 3 — Annotated.pdf](file://xn--file-8teyxesc7wj3ah3hbg6dhm%23:~:text=%20use%20k%20%201,for%20training,%201%20for%20test-ke92dbvd/)). Finally, test the chosen model on a held-out test set for an unbiased performance estimate.
- **Empirical vs. Population Risk:** _Empirical risk_ = training loss (error on training data); _Population risk_ = expected loss on the true data distribution (approximated by test error). Empirical risk is an optimistic estimate of true error when the model overfits. Generally, with an effective model, training error $\le$ test error, but due to random fluctuations empirical risk can occasionally be higher or lower than population risk ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=,lower%20than%20the%20population%20risk/)) ([midterm_practice_soln - Copy.pdf](file://file-gyheq2b7g3xp96bhewpdtk%23:~:text=empirical%20risk%20is%20a%20random,or%20smaller%20depending%20on%20chance/)). What matters is keeping the gap between train and test loss small (good generalization).
- **Regularization:** Techniques to discourage overfitting by penalizing model complexity. Add a term to the loss $L(\boldsymbol{\beta})$ that grows with model complexity (e.g. large weights).
    - **L2 Regularization (Ridge):** Add $\lambda |\boldsymbol{\beta}|_2^2 = \lambda\sum_j \beta_j^2$ to the loss ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=types%20of%20regularization:/)) ([lec4_annotated.pdf](file://xn--file-5vhyfd1zvbldtluwvaeuz8%23:~:text=%20l1%20regularization:%20adds%20the,weights%20to%20the%20loss%20function-ys09e/)). Encourages smaller parameter values (shrinks coefficients towards 0). It keeps all features but reduces their influence. Closed-form solution for ridge exists for linear regression: $\hat{\boldsymbol{\beta}}=(X^T X + \lambda I)^{-1}X^T \mathbf{y}$. Increasing $\lambda$ _decreases_ the norm of $\beta$ (weights get smaller) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=22%20with%20different%20positive%20values,let%20-oe2dwbx0dk479s78aba/)) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=,then%20%20122%20%20-ip3bm24758bna08fgai862b/)) and _increases_ training loss (imposing more bias) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=,1%20%20y22%20%20x-1d2bq316r3iar3aia870b/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=parameter%20always%20leads%20to%20higher,it%20might%20improve%20test%20loss/)).
    - **L1 Regularization (Lasso):** Add $\lambda |\boldsymbol{\beta}|_1 = \lambda\sum_j |\beta_j|$ ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=types%20of%20regularization:/)) ([lec4_annotated.pdf](file://xn--file-5vhyfd1zvbldtluwvaeuz8%23:~:text=%20l1%20regularization:%20adds%20the,weights%20to%20the%20loss%20function-ys09e/)). Encourages sparsity – many coefficients driven exactly to 0 (feature selection). No closed form solution (optimization via e.g. coordinate descent), but effect: larger $\lambda$ yields more zeros (simpler model). Like ridge, higher $\lambda$ increases bias (training error) and lowers variance. _(Elastic Net combines L1 and L2 terms.)_
    - **Effect on Weights and Loss:** If $\lambda_1 \ge \lambda_2$, then ridge-optimal weights satisfy $|\beta^_(\lambda_1)|_2 \le |\beta^_(\lambda_2)|_2$ (stronger regularization ⇒ smaller weight norm) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=22%20with%20different%20positive%20values,let%20-oe2dwbx0dk479s78aba/)) ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=,then%20%20122%20%20-ip3bm24758bna08fgai862b/)), and training error is higher $L(\beta^_(\lambda_1)) \ge L(\beta^_(\lambda_2))$ ([Homework 2 ML — Questions.pdf](file://xn--file-buuqk4gerrl6pwcgecf1lu%23:~:text=,1%20%20y22%20%20x-1d2bq316r3iar3aia870b/)) ([Homework 2 ML — Questions.pdf](file://file-buuqk4gerrl6pwcgecf1lu%23:~:text=parameter%20always%20leads%20to%20higher,it%20might%20improve%20test%20loss/)). Lasso shows a similar pattern (larger $\lambda$ yields no lower weight norm and no lower training error than smaller $\lambda$ – it also monotonically shrinks weights). The key benefit is improved test performance when the true function is simpler than an overfit model.
- **Bias-Variance Trade-off:** Increasing model complexity (or decreasing regularization) tends to decrease bias (fit training data more closely) but increase variance (sensitivity to random noise). Simpler models or stronger regularization do the opposite (high bias, low variance). The optimal model balances the two. For example, a deep tree has low bias but high variance, while a shallow tree or linear model has higher bias but lower variance. Techniques like bagging reduce variance, while boosting can reduce bias. Regularization increases bias but can dramatically reduce variance, yielding a lower overall error if the unregularized model was overfitting ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=bias/)) ([lec4_annotated.pdf](file://file-5vhyfd1zvbldtluwvaeuz8%23:~:text=bias,concept%20in%20machine%20learning/)). Use validation data or CV to find the setting that minimizes total error.