---
tags:
  - ML/Week1
---

## Concise


## Prompting
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.30.50 AM.jpg| 750]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.32.33 AM.jpg| 750]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.33.03 AM.jpg| 750]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.34.30 AM.jpg| 750]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.37.36 AM.jpg| 750]]

_“This is the entire job of any supervised learning algorithm”_—that is, to define a model, pick a suitable loss function, and then find the parameters that minimize that loss on the training data. This minimization process is known as **Empirical Risk Minimization**.

![[z/z ScreenShots/Screenshot 2025-02-26 at 4.40.27 AM.jpg| 750]]
This is the extension of the single-variable calculus idea to multiple variables: instead of just one derivative, you have a system of equations (one partial derivative per parameter). Solving that system gives you the parameter values that (ideally) minimize the function.

![[z/z ScreenShots/Screenshot 2025-02-26 at 4.40.50 AM.jpg| 750]]

## Minimizing Squared Loss for Regression

![[z/z ScreenShots/Screenshot 2025-02-26 at 4.44.09 AM.jpg]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.44.34 AM.jpg]]![[z/z ScreenShots/Screenshot 2025-02-26 at 4.45.34 AM.jpg]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.45.56 AM.jpg]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 4.46.05 AM.jpg]]
![[z/z ScreenShots/Screenshot 2025-02-26 at 5.56.47 AM.jpg| 750]]
