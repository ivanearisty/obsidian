<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

    <title></title>
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/black.css" id="theme" />
    <link rel="stylesheet" href="plugin/highlight/zenburn.css" />
	<link rel="stylesheet" href="css/layout.css" />
	<link rel="stylesheet" href="plugin/customcontrols/style.css">



    <script defer src="dist/fontawesome/all.min.js"></script>

	<script type="text/javascript">
		var forgetPop = true;
		function onPopState(event) {
			if(forgetPop){
				forgetPop = false;
			} else {
				parent.postMessage(event.target.location.href, "app://obsidian.md");
			}
        }
		window.onpopstate = onPopState;
		window.onmessage = event => {
			if(event.data == "reload"){
				window.document.location.reload();
			}
			forgetPop = true;
		}

		function fitElements(){
			const itemsToFit = document.getElementsByClassName('fitText');
			for (const item in itemsToFit) {
				if (Object.hasOwnProperty.call(itemsToFit, item)) {
					var element = itemsToFit[item];
					fitElement(element,1, 1000);
					element.classList.remove('fitText');
				}
			}
		}

		function fitElement(element, start, end){

			let size = (end + start) / 2;
			element.style.fontSize = `${size}px`;

			if(Math.abs(start - end) < 1){
				while(element.scrollHeight > element.offsetHeight){
					size--;
					element.style.fontSize = `${size}px`;
				}
				return;
			}

			if(element.scrollHeight > element.offsetHeight){
				fitElement(element, start, size);
			} else {
				fitElement(element, size, end);
			}		
		}


		document.onreadystatechange = () => {
			fitElements();
			if (document.readyState === 'complete') {
				if (window.location.href.indexOf("?export") != -1){
					parent.postMessage(event.target.location.href, "app://obsidian.md");
				}
				if (window.location.href.indexOf("print-pdf") != -1){
					let stateCheck = setInterval(() => {
						clearInterval(stateCheck);
						window.print();
					}, 250);
				}
			}
	};


        </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides"><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

# Urban Time Traveler
## Tracking the Evolution of Pedestrian Infrastructure
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

**The Idea:** 
A "Time Machine" for urban planners and researchers to visualize how pedestrian infrastructure (sidewalks, crosswalks) has evolved.
<small>
**Scope:**
- **Focus Area:** Central Brooklyn
- **Time Range:** Historical analysis of as many years we can find.
- **Output:** A fully interactive web dashboard comparing detected infrastructure against official city records.
- 
</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

### Key Questions
<small>

1. **Quantifying Change**: How has the total length and area of pedestrian space changed over the last decade?

2.  **Detection Accuracy:** Can computer vision models (`tile2net`) accurately extract infrastructure from historical aerial imagery compared to official datasets?
3.  **Gap Analysis:** Where are the discrepancies between physical infrastructure (satellite view) and city records?
4.  **Walkability Evolution:** Have specific neighborhoods seen measurable improvements in walkability metrics?

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Data Acquisition Sources

We aggregated data from three primary sources to build our ground truth and inference inputs:

<small>

1.  **NYC Planimetrics:** Official vector data (Shapefiles/GDB) used for validation (Years: 1996, 2004, 2014, 2022).
2.  **OpenStreetMap (OSM):** Contextual street network data.
3.  **NYCSidewalks:** Additional reference datasets.
4.  **NYC Map Tiles:** Historical aerial imagery tiles (XYZ service).

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Data Acquisition Issues

Obtaining high-resolution historical data was not straightforward.

<small>

*   **Rate Limits:** NYC GIS servers throttled our batch download requests.
*   **Bandwidth:** High-res imagery requires massive bandwidth; local downloading took hours.
*   **File Sizes:** Reference Shapefiles and GDBs were gigabytes in size, crashing local scripts.
*   **Missing Tiles:** The silent killer of our inference pipeline.

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## The "Antipattern"

<div class="" style="" >

<img src="z/Screenshot 2025-12-01 at 4.00.30 PM.png" alt="" style="width: 300px; object-fit: fill">

<img src="z/Screenshot 2025-12-01 at 4.01.12 PM.png" alt="" style="width: 70px; object-fit: fill">

<img src="z/Screenshot 2025-12-01 at 4.03.18 PM.png" alt="" style="width: 50px; object-fit: fill">

</div>

 

<small>

We allowed our inference VM to fetch tiles dynamically. When the external server 404'd, 403'd, or timed out, the model ran inference on **black/empty tiles**.

*   **Verify Inputs First:** Never let expensive GPUs run on unvalidated data.
*   **Pre-download & Stitch:** Ensure all assets are valid on disk before spinning up inference.
*   **Cost Implication:** We burned compute credits processing "nothingness."

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Remedies & Scaling Strategy

<small>

To overcome these bottlenecks, we pivoted our approach:

1.  **Proof of Concept:** Successfully ran the model on a single block in Brooklyn to validate the pipeline.
2.  **Controlled Expansion:** Expanded to a specific bounding box in Brooklyn, capping raw file sizes to ~1GB.
3.  **Cloud Orchestration:** Created batch jobs to process large areas in parallel.

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Current Status & Cost

**The Good:**
We have a working pipeline, finished inference on all of our data, and a functional frontend.

**The Bad (The Cost):**
If we made a mistake it'll be pricey.
*   **Inference Cost:** ~$4.00 per year of data processed.
*   **Time:** ~40 minutes runtime per year. (all of brooklyn takes about 3 hours)
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Frontend Challenges: Scaling Logic

<small>

With a small dataset, everything was fine. With full Brooklyn data:
*   **RAM Constraints:** Browser memory spikes when loading 2GB+ GeoJSON files.
*   **Render Lag:** Mapbox GL struggles with thousands of dynamic vectors.

**Solution:**
1. Looking for ways to simplify geometry to ensure smooth 60FPS interaction on the slider.
2. Figuring out whether we can store the data in a more performant database that's not just a big js array

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Frontend Breakdown

Our React application is designed for analysis, not just viewing.

<small>

**Core Capabilities:**
1.  **Temporal Slider:** Smooth transition between 2014, 2016, and 2018.
2.  **Change Summary:** Real-time diffing (Added vs. Removed vs. Unchanged segments).
3.  **Metrics Dashboard:** Charts visualizing net growth in infrastructure length.
4.  **Validation Dashboard:** Runs validation of model outputs vs accepted data.

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Planned Changes for Final Report

<small>

1.  **Performance:** Figure out a way to replace raw GeoJSON, solving the RAM crash issue.
2.  **Accuracy:** Tune the spatial tolerance parameters in `referenceData.js` to better handle GPS drift in historical imagery.
3.  **Control:** Add user defined controls for spatial tolerance parameters in the UI.

</small>
</div></script></section><section  data-markdown><script type="text/template"><!-- .slide: class="drop" -->
<div class="" style="position: absolute; left: 0px; top: 0px; height: 700px; width: 960px; min-height: 700px; display: flex; flex-direction: column; align-items: center; justify-content: center" absolute="true">

## Conclusion

We built the machine.
Now we need to tweak it.

By solving the data engineering bottlenecks and validating our inputs, the **Urban Time Traveler** is ready to process the full historical archive and provide concrete answers to our research questions on urban evolution.
</div></script></section></div>
    </div>

    <script src="dist/reveal.js"></script>

    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/zoom/zoom.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/math/math.js"></script>
	<script src="plugin/mermaid/mermaid.js"></script>
	<script src="plugin/chart/chart.min.js"></script>
	<script src="plugin/chart/plugin.js"></script>
	<script src="plugin/menu/menu.js"></script>
	<script src="plugin/customcontrols/plugin.js"></script>

    <script>
      function extend() {
        var target = {};
        for (var i = 0; i < arguments.length; i++) {
          var source = arguments[i];
          for (var key in source) {
            if (source.hasOwnProperty(key)) {
              target[key] = source[key];
            }
          }
        }
        return target;
      }

	  function isLight(color) {
		let hex = color.replace('#', '');

		// convert #fff => #ffffff
		if(hex.length == 3){
			hex = `${hex[0]}${hex[0]}${hex[1]}${hex[1]}${hex[2]}${hex[2]}`;
		}

		const c_r = parseInt(hex.substr(0, 2), 16);
		const c_g = parseInt(hex.substr(2, 2), 16);
		const c_b = parseInt(hex.substr(4, 2), 16);
		const brightness = ((c_r * 299) + (c_g * 587) + (c_b * 114)) / 1000;
		return brightness > 155;
	}

	var bgColor = getComputedStyle(document.documentElement).getPropertyValue('--r-background-color').trim();
	var isLight = isLight(bgColor);

	if(isLight){
		document.body.classList.add('has-light-background');
	} else {
		document.body.classList.add('has-dark-background');
	}

      // default options to init reveal.js
      var defaultOptions = {
        controls: true,
        progress: true,
        history: true,
        center: true,
        transition: 'default', // none/fade/slide/convex/concave/zoom
        plugins: [
          RevealMarkdown,
          RevealHighlight,
          RevealZoom,
          RevealNotes,
          RevealMath.MathJax3,
		  RevealMermaid,
		  RevealChart,
		  RevealCustomControls,
		  RevealMenu,
        ],


    	allottedTime: 120 * 1000,

		mathjax3: {
			mathjax: 'plugin/math/mathjax/tex-mml-chtml.js',
		},
		markdown: {
		  gfm: true,
		  mangle: true,
		  pedantic: false,
		  smartLists: false,
		  smartypants: false,
		},

		mermaid: {
			theme: isLight ? 'default' : 'dark',
		},

		customcontrols: {
			controls: [
			]
		},
		menu: {
			loadIcons: false
		}
      };

      // options from URL query string
      var queryOptions = Reveal().getQueryHash() || {};

      var options = extend(defaultOptions, {"width":960,"height":700,"margin":0.04,"controls":true,"progress":true,"slideNumber":true,"transition":"slide","transitionSpeed":"default"}, queryOptions);
    </script>

    <script>
      Reveal.initialize(options);
    </script>
  </body>

  <!-- created with Advanced Slides -->
</html>
